{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvZNupW5M1ys"
   },
   "source": [
    "# Lecture 20: Bayesian inference\n",
    "\n",
    "Last time we talked about maximum likelihood inference, a method to estimate parameters from data. Using this method we would be able to find the parameters that maximize the likelihood, or in other words, the probability of observing our data.\n",
    "\n",
    "[Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference) takes a different approach: instead of picking out just the parameters that maximize the likelihood, we average over the set of all parameters, weighted by their likelihood of producing the data. Bayesian inference also allows us to define what is called a [prior distribution](https://en.wikipedia.org/wiki/Prior_probability) for the parameters. The prior distribution quantifies our knowledge (or expectation) of the parameter values before looking at the data. The parameters that we infer are then influenced by both the likelihood and the prior distribution.\n",
    "\n",
    "\n",
    "### Example: Estimating the bias of a coin flip\n",
    "\n",
    "Let's consider a process with a binary outcome. One example of this would be a coin flip: the coin can either land on heads (1) or tails (0). Here, which outcome we refer to as 1 and which we refer to as 0 is not important.\n",
    "\n",
    "Our task is to estimate the probability that we get a 1 or a 0 from a limited set of data. Because the probabilities must sum to one, there is really only one parameter to estimate. We could choose that parameter to be the probability of getting a 1 with each \"flip\", which we'll call $p$. This is analogous to an Ising spin coupled to an external magnetic field. In that case, the strength and direction of the magnetic field corresponds to the bias on the coin.\n",
    "\n",
    "If we were doing maximum likelihood inference, we would compute the likelihood of getting a set of data, and then we would look for the value of $p$ that is most likely to yield the data that we observed. But, as we saw, this leads to counterintuitive results especially when we have little data to work with. Here, we'll take the Bayesian approach instead: averaging over the different possible values of $p$, **weighted** by the probability that they produced the data that we observed.\n",
    "\n",
    "First, let's generate a set of data to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "WESNiRsEM1yt",
    "outputId": "299344dd-ca92-4dcd-c91b-9f272d6a3cea"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "\n",
    "p_true    = 0.6\n",
    "n_samples = 10\n",
    "\n",
    "data = rng.choice(range(2), p=[1 - p_true, p_true], size=n_samples)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwrhSycWM1yw"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jj--50KQM1yz"
   },
   "source": [
    "### Computing the likelihood\n",
    "\n",
    "How likely would we be to generate the above data for each possible value of $p$? The probability of each 1 would be $p$, and the probability of each 0 would be $1-p$. Because each event is independent, the total probability of the data is just the product of these individual probabilities.\n",
    "\n",
    "Below, let's write a function to compute the likelihood of the data as a function of $p$ and let's plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gVNZOrY5M1y0",
    "outputId": "9920099c-9970-4f5b-893a-f6d7dc5c4527"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def likelihood(x, data):\n",
    "    # Fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGBEU7v8M1y3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPI4h6vGM1y6"
   },
   "source": [
    "### Normalization\n",
    "\n",
    "In Bayesian inference, we would think of the likelihood as something like a probability distribution for the **parameters** -- $p$, in this case. However, this \"distribution\" isn't properly normalized. To do this we would need to integrate the likelihood over all possible values of $p$, which ranges from $\\left[0, 1\\right]$.\n",
    "\n",
    "Once we have the normalization, we can compute the \"probability\" of different values of $p$, as well as the expected value of $p$ when averaged over the likelihood. This latter quantity is referred to as the posterior mean (of $p$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "lERitbWsM1y7",
    "outputId": "f68bd8dd-1fa2-48ff-b719-cdbe91bcad18"
   },
   "outputs": [],
   "source": [
    "from scipy import quad\n",
    "\n",
    "norm = quad(likelihood, 0, 1, args=(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frJWTXqrM1y9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "th5En8gYM1zA"
   },
   "source": [
    "### Adding a prior distribution\n",
    "\n",
    "In the previous example, we did not explicitly include a prior distribution $P_{\\rm prior}(p)$ that quantifies the expectations that we have about the value of $p$ *before* seeing the data. In this case, we have what's called an implicit prior for $p$. By default, we assume that all possible values in the range from zero to one are equally probable.\n",
    "\n",
    "Let's include an explicit prior distribution now. In particular, let's assume that the probability of getting a 1 and a 0 are normally distributed around one half. In other words, we would assume that our \"coin\" is probably fair, but that plausible values of $p$ could be in some range around 0.5. We can adjust the width of this distribution to quantify our confidence that $p$ should be around 0.5.\n",
    "\n",
    "The posterior probability of different values of $p$ is then given by the (normalized) product of the likelihood and the prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ffClai9M1zB"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxXiftxpM1zE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-d34icHWM1zH"
   },
   "source": [
    "### Inference with and without the prior\n",
    "\n",
    "Below, let's compare our inferences for different number of samples and different widths of the prior distribution, including an implicit, uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "De5hK_4_M1zI"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1IvAVByM1zL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of lecture-17.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
